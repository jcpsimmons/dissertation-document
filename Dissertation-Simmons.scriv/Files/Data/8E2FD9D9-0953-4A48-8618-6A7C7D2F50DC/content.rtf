{\rtf1\ansi\ansicpg1252\cocoartf1671
{\fonttbl\f0\froman\fcharset0 TimesNewRomanPSMT;\f1\froman\fcharset0 TimesNewRomanPS-ItalicMT;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f0\fs24 \cf0 The concept of postmodality was at the core of 
\f1\i MKUltra
\f0\i0  although it was not commonly understood by participants. Paring down the mediums used for 
\f1\i Abstruse
\f0\i0  would help to more clearly communicate this concept, however I felt as though to make it abundantly clear I needed higher order integration of VR and audio. 
\f1\i MKUltra
\f0\i0  featured lower order integration of many different mediums, briefly put, many mediums emerging from the same conceptual starter. 
\f1\i Abstruse
\f0\i0  would attempt the opposite strategy: just two mediums featuring a very high order integration to the core concept of postmodality. The questions then were, how to more sufficiently link VR and audio, and how to best present the linkage.\
Typically, in an improvisation between two musicians there is bidirectional communication. This bidirectional communication is arguably one of the facets of musical improvisation that makes it enjoyable, and complex. Considering just the music and VR elements, there was only unidirectional communication between musician (myself) and the VR simulation (headset wearer). The headset wearer could navigate the experience in their own way by moving in 360 degrees, but other participants, not wearing the headset, couldn\'92t see the output of this navigation, and it only controlled the VR environment. In order to establish bidirectional communication between headset wearer and musician I designed and implemented a number of interventions.\
Firstly the VR headset wearer had to graduate from participant to performer. I attempted this during the production of 
\f1\i MKUltra 
\f0\i0 by placing the headset wearer at the \'93front\'94 of the performance space next to my synthesizer setup, but most participants concluded that the headset makes any participant look unflattering, such is the current state of VR hardware\'85 For 
\f1\i Abstruse 
\f0\i0 I decided to place the headset wearer behind the audience seating so that they were out of view. In order to highlight the headset wearer\'92s participation, the live output of the headset was displayed on a floor-to-ceiling wall projection only a few yards in front of the audience (a technique to more fully immerse audience members with just a 2D display). \
In addition to this enhanced display output, I coded a script to be executed inside of the VR software that would send packets of OSC data over an ad-hoc WiFi network containing pitch, roll, and yaw data of the headset. These packets of data would be received by custom software running inside of an instance of MaxMSP within Ableton Live that would control binaural spatialization of a virtual listener, thus changing the apparent location of the different granular sound sources in real-time and under the control of the headset wearer. \
To complete the postmodal improvisation loop, I, as performer, would be able to view how the headset wearer navigated the space by means of the projection screen, and respond in real-time to scene, and navigation-style changes. In turn, my musical changes would inform the headset wearer how to navigate the virtual space. In this way, a postmodal realization of existing music-only improvisational models such as Burrows\'92s Mediational Triangle (Burrows 2004, 3), or even the model of Group Cognition (Ibid, 8) would be reached. More user research is necessary to determine which model 
\f1\i Abstruse
\f0\i0  fits into better, although the components and general relations exist for both.}