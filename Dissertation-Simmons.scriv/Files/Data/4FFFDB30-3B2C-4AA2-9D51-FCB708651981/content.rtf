{\rtf1\ansi\ansicpg1252\cocoartf1671
{\fonttbl\f0\froman\fcharset0 TimesNewRomanPSMT;\f1\froman\fcharset0 TimesNewRomanPS-ItalicMT;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f0\fs24 \cf0 I utilized the framework most succinctly put by Campbell in my development of the audio responsive VR environment 
\f1\i Spektra
\f0\i0 . I found Campbell\'92s framework to be particularly useful when foraying into this mostly foreign and uncharted territory of development. Virtual reality, as explained throughout this writing, has a lineage, a set of preconceived notions and \'93best practices\'94, but at its current state offers to wipe the slate clean again. \
Developing modern software takes the form of the box it is poured into. McLuhan\'92s The Medium is the Message provides sufficient evidence to back this claim. The hardware used for the project consisted of an HTC Vive headset, two HTC tracking base stations, and a modern computer using a Radeon RX480 graphics card. More peripherals were available, but since the hardware had to be moved from the laboratory into the performance space, economy of size was of the utmost importance.\
The first prototype I constructed to test audio-tracking in VR was a complex and obtuse pipeline. The initial beta involved both my personal MacBook Air for musical information retrieval, and the aforementioned VR-capable PC for data sorting, mapping, and live VR rendering. I initially used various audio-tracking libraries in the MaxMSP programming language to perform basic musical feature extraction. This data was then scaled to values residing between 0.0 and 1.0, the resolution of this range consisting of single-precision floating-point numbers. I provided fine-resolution interpolation to this data to smooth transitions. The smoothed data was then packaged into OSC format and broadcast over a local, ad-hoc WiFi network being hosted by the rendering PC. \
The render PC was running Unreal Engine 4, a game engine software used for Realtime VR rendering. Unreal Engine 4 natively includes a graphical programming language called Blueprints for rapid prototyping. Using UE4-OSC, an \'93OSC plugin for Unreal Engine 4 to send and receive OSC messages with blueprints!\'94 (Buisson 2014) I built a Blueprints script to receive the OSC data coming from the laptop. Then I mapped this data to the Y position of 4 cubes spaced proportionally in virtual space and rendered using Unreal Engine.\
The prototype served as a working proof of concept, but the reliability of UE4-OSC and the use of an additional computer were both suboptimal features. At the time of this writing, the UE4-OSC code has not been updated on GitHub in over a year. Sometimes it would abruptly stop receiving data or behave in other unpredictable ways despite network speeds and fidelity that proved to be more than adequate in tests run outside of Unreal Engine. \
My decision to work towards having the entire tracking and simulation operation run on one machine was a more philosophical one. I was interested in using VR for many reasons. One of which was the solitary experience it provided. In my research, this sort of out-of-the-ordinary experience had in solitude seemed to resonate with many shamanic traditions written about by figures such as Terrence McKenna, Richard Alpert, and Timothy Leary. Honoring this choice of medium, I extrapolated that the entire simulation should be a solitary experience. Many well-meaning voices during this development stage urged me to make the experience social, to somehow include other musicians, or other virtual avatars, but I wanted to employ the use of abstract symbols, gleaned from mythological and psychedelic drug research, to provide the user with a more profound experience.\
I decided to continue to utilize musical information retrieval as a means of controlling the virtual world, but in honoring the solitary aesthetic suggested by the VR headset, I wanted the entire experience to be self-contained, imperceptible to anyone outside of the headset. Through using a virtual loopback audio interface freely available on Windows (VoiceMeeter [sic]), I was able to reroute local audio, in most test cases from the Spotify app running in the background, directly to the VR simulation, and also to the headset-wearer\'92s headphones.\
At this point of development, I began to work with a colleague in the program, and competent C# programmer, Mark Micchelli. Micchelli\'92s mastery of musical improvisation combined with his programming ability made him the perfect complement to the project. With Micchelli\'92s coding ability in C#, we were able to use the Unity game engine. Unity is comparable to Unreal Engine but does not natively include a visual scripting system. Thus, the game developer must use C# for all game scripting. After testing numerous candidate libraries, we settled upon one named \'93Reaktion\'94.\
Reaktion was developed by Keijiro Takahashi and is distributed freely on GitHub (Takahashi 2014). Takahashi works for Unity, and his GitHub is full of repositories of avant-garde Unity engine technology, and scrapped development trees from earlier ideas. It is an important resource for any modern Unity developer. Reaktion is a Unity library dedicated entirely to receiving OSC data and mapping it to various game parameters. Furthermore, the software is open-source, so with Micchelli\'92s coding abilities, the possibility of modding and interpreting Reaktion data was available. The resulting simulation went through many iterations of development.\
}